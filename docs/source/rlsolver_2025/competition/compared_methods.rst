Compared Methods
===============

This section lists all baseline and reference methods that will be compared against your submission. A brief description, advantages, and disadvantages are provided for each.

1. **Classical Heuristics**  
   - **Greedy Heuristic**  
     - **Description**: Sort edges by weight in descending order. Iteratively assign nodes to one of the two partitions if doing so increases the cut.  
     - **Advantages**: Very fast; trivial to implement.  
     - **Disadvantages**: Easily trapped in local optima; quality is often far from optimal on dense graphs.
   - **Local Search (LS)**  
     - **Description**: Start from a random initial cut. At each step, move a single node if it improves the cut value; repeat until no improvement is possible.  
     - **Advantages**: Typically outperforms pure greedy on small to medium graphs.  
     - **Disadvantages**: Performance heavily depends on the initial cut; may require many iterations.

2. **Metaheuristic Methods**  
   - **Simulated Annealing (SA)**  
     - **Description**: Similar to local search but accepts worse moves with a probability that decreases over time (temperature parameter).  
     - **Advantages**: Can escape local optima by allowing uphill moves early on.  
     - **Disadvantages**: Requires careful temperature scheduling; runtime can be large for good solutions.
   - **Genetic Algorithm (GA)**  
     - **Description**: Maintain a population of cuts, apply crossover and mutation operators, and select the best individuals over generations.  
     - **Advantages**: Explores a diverse set of solutions; good for global search.  
     - **Disadvantages**: Many hyperparameters (population size, crossover rate, mutation rate); can be slow to converge.

3. **Exact or MIP Solvers**  
   - **Gurobi (ILP or QUBO Formulation)**
     - **Description**: Formulate MaxCut as a binary quadratic integer program (QIP) or an ILP via linearization, solve with Gurobi to optimality or time limit.  
     - **Advantages**: Provides an optimality guarantee when it converges; widely used in industry.  
     - **Disadvantages**: Not scalable to large graphs (≥ 500 nodes) under reasonable time limits.
   - **CPLEX (ILP or QUBO)**
     - **Description**: Similar to Gurobi but using IBM’s CPLEX solver.  
     - **Advantages**: Comparable performance to Gurobi in many instances.  
     - **Disadvantages**: Same scalability constraints on large graphs.

4. **Graph Neural Network (GNN) Methods**  
   - **GCN + MIP Refinement**  
     - **Description**: Use a Graph Convolutional Network (GCN) to predict soft node assignments, then solve a small mixed-integer program to finalize the cut.  
     - **Advantages**: Leverages learned patterns; often yields good solutions on moderate-sized graphs.  
     - **Disadvantages**: Two-stage pipeline adds complexity; training can be time-consuming.
   - **Graph Attention Network (GAT) + Local Search**  
     - **Description**: Use GAT to produce initial node embedding, then apply local search to improve the cut.  
     - **Advantages**: Attention mechanism can capture important graph structure; local search refines results.  
     - **Disadvantages**: Requires tuning of both GAT and local search hyperparameters.

5. **Reinforcement Learning Methods**  
   - **DQN (Deep Q-Network)**  
     - **Description**: Model MaxCut as a sequential decision process. State = partial cut assignment, action = flip node partition, reward = increase in cut value.  
     - **Advantages**: Learns to make decisions that maximize long-term cut value; no human-crafted heuristics needed.  
     - **Disadvantages**: Training can be unstable; high variance; expensive on large graphs.
   - **GNN-PPO (Graph Neural Network + Proximal Policy Optimization)**  
     - **Description**: Use a GNN to encode the graph state and a PPO agent to decide node flips or assignments.  
     - **Advantages**: PPO is more stable than vanilla policy gradient; GNN captures graph structure.  
     - **Disadvantages**: Requires a large number of episodes to converge; sensitive to hyperparameters.
   - **DeepGL (Deep Graph Learning)**  
     - **Description**: A hybrid of GNN and classical local search. Pretrain a GNN to estimate node contributions, then use those estimates to guide a local search routine.  
     - **Advantages**: Balances learned heuristics with search-based refinement; often faster convergence.  
     - **Disadvantages**: Complexity increases due to mixing two paradigms; tuning required for both components.

6. **Comparison Table**  
  
+------------------------+-------------------------------+---------------------------------------------+
| Method                 | Advantages                    | Disadvantages                               |
+========================+===============================+=============================================+
| Greedy Heuristic       | Fast, easy to implement       | Prone to local optima                       |
+------------------------+-------------------------------+---------------------------------------------+
| Local Search (LS)      | Better quality than greedy    | Sensitive to initial solution               |
+------------------------+-------------------------------+---------------------------------------------+
| Simulated Annealing    | Can escape local optima       | Requires temperature tuning                 |
+------------------------+-------------------------------+---------------------------------------------+
| Gurobi                 | Optimality guarantee          | Not scalable to large graphs                |
+------------------------+-------------------------------+---------------------------------------------+
| GCN + MIP Refinement   | Learns graph patterns         | Two-stage pipeline complexity               |
+------------------------+-------------------------------+---------------------------------------------+
| DQN                    | No hand-crafted heuristics    | Unstable training, high variance            |
+------------------------+-------------------------------+---------------------------------------------+
| GNN-PPO                | Stable policy updates (PPO)   | Requires many training episodes             |
+------------------------+-------------------------------+---------------------------------------------+
| DeepGL                 | Combines learning + search    | Complex to tune; mixed methodology overhead |
+------------------------+-------------------------------+---------------------------------------------+